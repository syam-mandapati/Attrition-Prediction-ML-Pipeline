{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a811feb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- All imports consolidated here ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from scipy.stats import zscore\n",
    "from sklearn.feature_selection import RFE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from collections import Counter\n",
    "import joblib\n",
    "import plotly.express as px\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c9f808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the dataset\n",
    "df = pd.read_csv(\"WA_Fn-UseC_-HR-Employee-Attrition.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23352488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the first 5 rows of the dataframe\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09ef14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the summary statistics of the dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a804c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the summary statistics of the dataframe\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d98515a",
   "metadata": {},
   "source": [
    "<!-- Exploratory Data Analysis (EDA) is a crucial step in understanding the dataset before building any machine learning models. It helps in identifying patterns, spotting anomalies, and checking assumptions with the help of summary statistics and graphical representations. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d121ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of missing values per column\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percentage})\n",
    "print(missing_df[missing_df['Missing Values'] > 0])  # Only show columns with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebb8174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outlier detection using IQR method\n",
    "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "outlier_summary = {}\n",
    "for col in numerical_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[col] < lower) | (df[col] > upper)]\n",
    "    outlier_summary[col] = len(outliers)\n",
    "\n",
    "# Display number of outliers per numerical column\n",
    "outlier_df = pd.DataFrame.from_dict(outlier_summary, orient='index', columns=['Outlier Count'])\n",
    "print(outlier_df.sort_values(by='Outlier Count', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c642de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numerical_cols:\n",
    "    plt.figure(figsize=(5, 1.5))\n",
    "    plt.boxplot(df[col], vert=False)\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456f93a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization of target data relationships using matplotlib\n",
    "attrition_counts = df['Attrition'].value_counts()\n",
    "attrition_counts.plot(kind='bar', color=['skyblue', 'salmon'])\n",
    "plt.title(\"Attrition Distribution\")\n",
    "plt.xlabel(\"Attrition\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb252a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization of categorical columns\n",
    "categorical_cols = ['Department', 'JobRole', 'MaritalStatus']\n",
    "for col in categorical_cols:\n",
    "    df[col].value_counts().plot(kind='bar')\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82b1a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing distribution of numerical columns\n",
    "numerical_cols = ['Age', 'MonthlyIncome', 'DistanceFromHome']\n",
    "for col in numerical_cols:\n",
    "    plt.hist(df[col], bins=60, color='lightblue', edgecolor='black')\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade85041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'Attrition' is your target\n",
    "categorical_cols = df.select_dtypes(include='object').columns.drop('Attrition')\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7a8980",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "for col in categorical_cols:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.countplot(data=df, x=col, hue='Attrition')\n",
    "    plt.title(f'{col} vs Attrition')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c462dad0",
   "metadata": {},
   "source": [
    "#from above plots we can see lets say overtime catogory \"no\" are not attriting majoritily and over18 are also majoritily not attriting \n",
    "and in marital status \"single\" and \"married\" are not attriting majoritily and in job role \"sales executive\" are not attriting majoritily and in department \"sales\" are not attriting majoritily and in age group 30-40 are not attriting majoritily and in distance from home 1-5 km are not attriting majoritily and in monthly income 5000-10000 are not attriting majoritily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9479afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numerical_cols:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    for value in df['Attrition'].unique():\n",
    "        sns.kdeplot(df[df['Attrition'] == value][col], label=f\"Attrition: {value}\", fill=True)\n",
    "    plt.title(f'Distribution of {col} by Attrition')\n",
    "    plt.xlabel(col)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908bbe29",
   "metadata": {},
   "source": [
    "A KDE (Kernel Density Estimate) plot shows the probability distribution of a continuous variable. When used with a target like Attrition, it helps compare how a featureâ€™s values are distributed across different classes, highlighting patterns or separations.column wise we can see these are less seperable so may be combination of data will help in prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65195dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr(numeric_only=True)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.matshow(corr, fignum=1)\n",
    "plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
    "plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "plt.colorbar()\n",
    "plt.title(\"Correlation Matrix\", pad=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efd6dcc",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a55bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outlier replacement using Z-score and IQR methods\n",
    "from scipy.stats import zscore\n",
    "# Copy the dataframe to preserve original\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "for col in numerical_cols:\n",
    "    # Calculate Z-scores\n",
    "    z_scores = zscore(df_cleaned[col], nan_policy='omit')\n",
    "    \n",
    "    # Calculate IQR\n",
    "    Q1 = df_cleaned[col].quantile(0.25)\n",
    "    Q3 = df_cleaned[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Conditions for outliers using both Z-score and IQR\n",
    "    z_outliers = (z_scores > 3) | (z_scores < -3)\n",
    "    iqr_outliers = (df_cleaned[col] < lower_bound) | (df_cleaned[col] > upper_bound)\n",
    "    \n",
    "    # Final condition: outliers by both methods\n",
    "    outlier_mask = z_outliers & iqr_outliers\n",
    "    \n",
    "    # Replace with mean\n",
    "    mean_value = df_cleaned[col].mean()\n",
    "    df_cleaned.loc[outlier_mask, col] = mean_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d838d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding for categorical columns#model cannot understand categorical data, so we need to convert them into numerical format\n",
    "#booosting algorithms like XGBoost, LightGBM, and CatBoost can handle categorical data directly, but for other models, we need to encode them.\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Make a copy of the DataFrame\n",
    "df_encoded = df_cleaned.copy()\n",
    "\n",
    "# Identify categorical columns (excluding target if needed)\n",
    "categorical_cols = df_encoded.select_dtypes(include='object').columns\n",
    "\n",
    "# Initialize the encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Apply Label Encoding to each categorical column\n",
    "for col in categorical_cols:\n",
    "    df_encoded[col] = le.fit_transform(df_encoded[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f70786",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0658b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import seaborn as sns\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True)\n",
    "plt.title(\"Correlation Matrix with Annotations\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ecb8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8035abf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using plotly for interactive visualization\n",
    "\n",
    "\n",
    "# Compute correlation\n",
    "# corr = `df_encoded.corr()` is calculating the correlation matrix for the DataFrame `df_encoded`. This function computes pairwise correlation of columns, excluding NA/null values, and returns a DataFrame where the rows and columns are the variables and the values are the correlation coefficients.\n",
    "\n",
    "# Create interactive heatmap\n",
    "fig = px.imshow(df_encoded.corr(),\n",
    "                text_auto=True,  # show values on cells\n",
    "                color_continuous_scale='RdBu_r',\n",
    "                title=\"Correlation Matrix (Hover for Values)\",\n",
    "                labels=dict(color=\"Correlation\"))\n",
    "\n",
    "fig.update_layout(width=800, height=700)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23efbc7a",
   "metadata": {},
   "source": [
    "<!-- As per above correlation plot, we can see that many features are having correlation with other features but with target there are no features having atleast more than +/-0.5 correlation. This indicates that the features are not strongly correlated with the target variable, which is 'Attrition' in this case. This suggests that the model may need to rely on complex interactions between features to predict attrition effectively.And also you can see yearsAtCompany and yearsInCUrrentRole are having more than 0.7 correaltion so both are conveying same information even if you drop any ome feature it is okay to reduce dimensionality without losing much information. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c5c213",
   "metadata": {},
   "source": [
    "<!-- # Selecting top 20 features using RFE with RandomForestClassifier just to avoid overfitting and computational complexity -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8d8342",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define independent variables (X) and target (y)\n",
    "X = df_encoded.drop('Attrition', axis=1)\n",
    "y = df_encoded['Attrition']\n",
    "\n",
    "# Initialize model (Random Forest used here, you can try LogisticRegression, etc.)\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create the RFE model and select top 20 features\n",
    "rfe = RFE(estimator=model, n_features_to_select=20)\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = X.columns[rfe.support_]\n",
    "print(\"Top 20 Selected Features:\")\n",
    "print(selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5474a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_encoded[selected_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24c40f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I just want to use both distance based algorithms and tree based algorithms for classifcation tasks. So performing scaling on the data basically tree based algorithms not require scaling but no issue even if you apply scaling on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b67ca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#I have already replaced outliers in the data, so I am using StandardScaler here. if you want to use RobustScaler, you can uncomment the respective lines.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the numeric columns\n",
    "df_scaled = x.copy()\n",
    "df_scaled = scaler.fit_transform(df_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b866d1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Performing Train Test Split before applying SMOTETomek to avoid data leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize SMOTETomek with 50% sampling strategy means it make minority class samples equal to 50% of majority class samples\n",
    "smt = SMOTETomek(random_state=42, sampling_strategy=\"auto\")\n",
    "\n",
    "# Fit and resample the training data\n",
    "X_resampled, y_resampled = smt.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check class distribution after balancing\n",
    "from collections import Counter\n",
    "print(\"Original:\", Counter(y_train))\n",
    "print(\"Resampled:\", Counter(y_resampled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca3d95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": (LogisticRegression(max_iter=1000,class_weight='balanced'), {\n",
    "        'classifier__C': [0.1, 1, 10]\n",
    "    }),\n",
    "    \"Random Forest\": (RandomForestClassifier(class_weight='balanced'), {\n",
    "        'classifier__n_estimators': [50, 100],\n",
    "        'classifier__max_depth': [None, 10]\n",
    "    }),\n",
    "    \"SVM\": (SVC(probability=True), {\n",
    "        'classifier__C': [0.1, 1],\n",
    "        'classifier__kernel': ['linear', 'rbf']\n",
    "    }),\n",
    "    \"Naive Bayes\": (GaussianNB(), {\n",
    "        # No hyperparameters to tune for basic GaussianNB\n",
    "    }),\n",
    "    \"Decision Tree\": (DecisionTreeClassifier(), {\n",
    "        'classifier__max_depth': [None, 10, 20]\n",
    "    }),\n",
    "    \"KNN\": (KNeighborsClassifier(), {\n",
    "        'classifier__n_neighbors': [3, 5, 7]\n",
    "    }),\n",
    "    \"XGBoost\": (XGBClassifier(use_label_encoder=False, eval_metric='logloss'), {\n",
    "        'classifier__n_estimators': [50, 100],\n",
    "        'classifier__max_depth': [3, 6]\n",
    "    })\n",
    "}\n",
    "\n",
    "best_model_overall = None\n",
    "best_model_name = None\n",
    "best_model_accuracy = 0.0  # Start with 0 accuracy\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, (model, param_grid) in models.items():\n",
    "    pipe = Pipeline([\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "    if param_grid:\n",
    "        grid = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy')\n",
    "        grid.fit(X_train, y_train)\n",
    "        best_model = grid.best_estimator_\n",
    "        best_params = grid.best_params_\n",
    "    else:\n",
    "        pipe.fit(X_train, y_train)\n",
    "        best_model = pipe\n",
    "        best_params = \"Default\"\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Save result\n",
    "    results[name] = {\n",
    "        'accuracy': acc,\n",
    "        'best_params': best_params\n",
    "    }\n",
    "\n",
    "    print(f\"{name}: Accuracy = {acc:.4f}, Best Params = {best_params}\")\n",
    "\n",
    "    # Track the best model\n",
    "    if acc > best_model_accuracy:\n",
    "        best_model_accuracy = acc\n",
    "        best_model_overall = best_model\n",
    "        best_model_name = name\n",
    "\n",
    "# === Save the best model ===\n",
    "print(f\"\\nSaving best model: {best_model_name} with accuracy {best_model_accuracy:.4f}\")\n",
    "joblib.dump(best_model_overall, 'best_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcc2e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results).T\n",
    "results_df.sort_values(by='accuracy', ascending=False, inplace=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb75979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- To load the model later ---\n",
    "loaded_model = joblib.load('best_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88ffe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict on test set\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "\n",
    "# --- Print Classification Report ---\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# --- Plot Confusion Matrix ---\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=loaded_model.classes_,)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a053353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so for imbalanced data instead of focusing accuracy, we can focus on precision, recall, and F1-score.\n",
    "# These metrics give a better understanding of the model's performance on imbalanced datasets.\n",
    "# so now I will save the best model using recall of the class 1 (Attrition = Yes) as the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeecb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_recall = 0\n",
    "best_model_name = None\n",
    "best_model_obj = None\n",
    "results = {}\n",
    "\n",
    "for name, (model, param_grid) in models.items():\n",
    "    pipe = Pipeline([('classifier', model)])\n",
    "\n",
    "    if param_grid:\n",
    "        grid = GridSearchCV(pipe, param_grid, cv=5, scoring='recall', refit=True)\n",
    "        grid.fit(X_train, y_train)\n",
    "        best_estimator = grid.best_estimator_\n",
    "        best_params = grid.best_params_\n",
    "    else:\n",
    "        pipe.fit(X_train, y_train)\n",
    "        best_estimator = pipe\n",
    "        best_params = \"Default\"\n",
    "\n",
    "    y_pred = best_estimator.predict(X_test)\n",
    "\n",
    "    # Get recall for class 1\n",
    "    recall_class1 = recall_score(y_test, y_pred, pos_label=1)  # or pos_label='Yes' if labels are strings\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    results[name] = {\n",
    "        'accuracy': acc,\n",
    "        'recall_class1': recall_class1,\n",
    "        'best_params': best_params\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Recall (Attrition = Yes): {recall_class1:.4f}\")\n",
    "    print(f\"Best Params: {best_params}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f\"{name} Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # Save best model based on recall of class 1\n",
    "    if recall_class1 > best_recall:\n",
    "        best_recall = recall_class1\n",
    "        best_model_name = name\n",
    "        best_model_obj = best_estimator\n",
    "\n",
    "# === Save the best model ===\n",
    "joblib.dump(best_model_obj, 'best_model.pkl')\n",
    "print(f\"\\nBest Model Saved: {best_model_name} with Recall (Attrition = Yes) = {best_recall:.4f}\")\n",
    "loaded_model = joblib.load('best_model.pkl')\n",
    "y_pred_loaded = loaded_model.predict(X_test)\n",
    "print(\"Loaded Model Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_loaded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fab4d7",
   "metadata": {},
   "source": [
    "# Conclusion: Attrition Prediction Using Imbalanced Classification Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28bcea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # In this project, we tackled the challenge of predicting attrition using imbalanced classification models. Whether itâ€™s employee attrition or customer churn, the cost of missing true positives (i.e., not identifying someone who is likely to leave) is high. This makes recall a far more important metric than accuracy alone.\n",
    "\n",
    "# # ðŸ” Summary of Work:\n",
    "# # We trained several models including Logistic Regression, Random Forest, and others.\n",
    "\n",
    "# # Applied SMOTETomek to handle severe class imbalance.\n",
    "\n",
    "# # Evaluated models using Confusion Matrix, Classification Report, and business-critical metrics like Recall, Precision, and F1-score.\n",
    "\n",
    "# # Best-performing model was saved using joblib for future use.\n",
    "\n",
    "# # ðŸ“‰ Why Accuracy Was Misleading:\n",
    "# # Although some models showed up to 85% accuracy, the true negatives were very low, indicating that many churned individuals were misclassified. This reinforced that:\n",
    "\n",
    "# # High accuracy â‰  Good model in imbalanced classification.\n",
    "\n",
    "# # âœ… Best Model Observations:\n",
    "# # Logistic Regression gave the best recall score for the attrition class.\n",
    "\n",
    "# # However, it still had many false negatives, which can be costly in real-world business scenarios.\n",
    "\n",
    "# # Simply applying class balancing is not always sufficientâ€”especially when the data is not easily separable.\n",
    "\n",
    "# # ðŸ“Œ What to Focus on in Real-World Attrition Use Cases:\n",
    "# # Business Context Matters:\n",
    "\n",
    "# # In attrition prediction, recall for the positive class (Attrition = Yes) is often more important than precision.\n",
    "\n",
    "# # It's better to mistakenly target a non-churner than to miss a real churn risk.\n",
    "\n",
    "# # Advanced Techniques to Improve Results:\n",
    "\n",
    "# # Feature Engineering: Derive new features that better capture behavior leading to attrition.\n",
    "\n",
    "# # Cost-sensitive learning: Penalize false negatives more heavily during training.\n",
    "\n",
    "# # Model Ensemble & Threshold Tuning: Tune the decision threshold based on cost-benefit analysis or try ensemble models with different voting schemes.\n",
    "\n",
    "# # More Data: Sometimes data quality or quantity is the limiting factorâ€”request more samples if possible.\n",
    "\n",
    "# # Start With EDA, Always:\n",
    "\n",
    "# # Understand feature distributions, patterns, and imbalances.\n",
    "\n",
    "# # Identify potential data leaks, outliers, and missing values before modeling.\n",
    "\n",
    "# # ðŸ“¦ Final Note\n",
    "# # Attrition prediction is more than just a machine learning taskâ€”itâ€™s a strategic business problem. A well-tuned model can help proactively retain employees or customers, saving significant costs and improving long-term relationships.\n",
    "\n",
    "# # Always align model performance with business KPIs, not just technical metrics.\n",
    "\n",
    "#If it is customer data I would have requested for more data and used more advanced techniques like feature engineering, cost-sensitive learning, and ensemble methods to improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaa9a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ðŸ™Œ All the Best!\n",
    "# Wishing you success in your data science journey.\n",
    "# Stay curious, keep building, and enjoy the ride!\n",
    "\n",
    "# # Happy Coding & May Your Models Always Converge! ðŸš€ðŸ¤–\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ca0198",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87a3342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb10695",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
